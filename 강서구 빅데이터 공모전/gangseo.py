#%%
import pandas as pd
df2020 = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/2020ì í¬.csv", encoding='cp949')
# í–‰ì •ë™ ì½”ë“œê°€ '1150'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë°ì´í„° í•„í„°ë§
df2020 = df2020[df2020["í–‰ì •ë™_ì½”ë“œ"].astype(str).str.startswith("1150")]
# ë¶„ê¸°ë³„ + í–‰ì •ë™ì½”ë“œë³„ ì í¬ìˆ˜ í•©ê³„ ê³„ì‚°
df2020 = df2020.groupby(["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"], as_index=False)["ì í¬_ìˆ˜"].sum()

# %%
df2021 = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/2021ì í¬.csv", encoding='cp949')
df2021 = df2021[df2021["í–‰ì •ë™_ì½”ë“œ"].astype(str).str.startswith("1150")]
df2021 = df2021.groupby(["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"], as_index=False)["ì í¬_ìˆ˜"].sum()
df2022 = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/2022ì í¬.csv", encoding='cp949')
df2022 = df2022[df2022["í–‰ì •ë™_ì½”ë“œ"].astype(str).str.startswith("1150")]
df2022 = df2022.groupby(["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"], as_index=False)["ì í¬_ìˆ˜"].sum()
df2023 = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/2023ì í¬.csv", encoding='cp949')
df2023 = df2023[df2023["í–‰ì •ë™_ì½”ë“œ"].astype(str).str.startswith("1150")]
df2023 = df2023.groupby(["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"], as_index=False)["ì í¬_ìˆ˜"].sum()
df2024 = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/2024ì í¬.csv", encoding='cp949')
df2024 = df2024[df2024["í–‰ì •ë™_ì½”ë“œ"].astype(str).str.startswith("1150")]
df2024 = df2024.groupby(["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"], as_index=False)["ì í¬_ìˆ˜"].sum()
# %%
df_all = pd.concat([df2020, df2021, df2022, df2023, df2024], ignore_index=True)
df_all = df_all.sort_values(by=["ê¸°ì¤€_ë…„ë¶„ê¸°_ì½”ë“œ", "í–‰ì •ë™_ì½”ë“œ"])
# %%
df_all.to_csv("ê°•ì„œêµ¬ì í¬ìˆ˜.csv", encoding='cp949')



# %% í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª¨ìŒ ë° ê·¸ë˜í”„ ê¹¨ì§ ë°©ì§€
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from tslearn.metrics import cdist_dtw
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MinMaxScaler
from statsmodels.nonparametric.smoothers_lowess import lowess
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KernelDensity
from sklearn.neighbors import NearestNeighbors
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


# %% ê°•ì„œêµ¬ ë°ì´í„° ëª¨ìŒ ì½ê¸°
gangseo = pd.read_csv("C:/Users/jeongmin/Downloads/vscode/ê°•ì„œêµ¬.csv", encoding='utf-8')

# %% ë™ë³„ë©´ì (ì œê³±kmë‹¨ìœ„) ì´ìš©í•œ ë°€ë„ ê³„ì‚°
area_df = pd.DataFrame({
    "í–‰ì •ë™ëª…": ["ì—¼ì°½ë™","ë“±ì´Œ1ë™","ë“±ì´Œ2ë™","ë“±ì´Œ3ë™","í™”ê³¡ë³¸ë™","í™”ê³¡2ë™","í™”ê³¡3ë™","í™”ê³¡4ë™","í™”ê³¡6ë™","í™”ê³¡8ë™",
             "ê°€ì–‘1ë™","ê°€ì–‘2ë™","ê°€ì–‘3ë™","ë°œì‚°1ë™","ê³µí•­ë™","ë°©í™”1ë™","ë°©í™”2ë™","ë°©í™”3ë™","í™”ê³¡1ë™","ìš°ì¥ì‚°ë™"],
    "ë©´ì ": [1.74,0.64,0.92,0.79,0.98,0.45,0.53,0.82,1.11,0.53,
           4.7,1,0.5,2.94,10.87,1.48,6.41,2.55,1.12,1.36]
})

gangseo = gangseo.merge(area_df, on="í–‰ì •ë™ëª…", how="left")
gangseo["ìœ ë™ì¸êµ¬_ë°€ë„"] = gangseo["ì´_ìœ ë™ì¸êµ¬_ìˆ˜"] / gangseo["ë©´ì "]
for col in ["ìƒì£¼ì¸êµ¬", "í‰ì¼ìœ ë™ì¸êµ¬", "ì£¼ë§ìœ ë™ì¸êµ¬", "ì§‘ê°ì‹œì„¤", "ë²„ìŠ¤ì •ê±°ì¥ìˆ˜", "ì í¬ìˆ˜"]:
    gangseo[f"{col}_ë°€ë„"] = gangseo[col] / gangseo["ë©´ì "]
final = gangseo.drop(columns=["ë©´ì ", "ì í¬ìˆ˜", "ìƒì£¼ì¸êµ¬", "ì´_ìœ ë™ì¸êµ¬_ìˆ˜", "í‰ì¼ìœ ë™ì¸êµ¬", "ì£¼ë§ìœ ë™ì¸êµ¬", "ì§‘ê°ì‹œì„¤", "ë²„ìŠ¤ì •ê±°ì¥ìˆ˜"])

final




# %% í”¼ë²—: í–‰=í–‰ì •ë™ëª…, ì—´=date, ê°’=ì§€ì¶œì´ê¸ˆì•¡
final["date"] = final["date"].astype(int)
pivot = final.pivot_table(index="í–‰ì •ë™ëª…", columns="date", values="ì§€ì¶œì´ê¸ˆì•¡", aggfunc="sum")
pivot = pivot.sort_index(axis=1)
ts = pivot.dropna(axis=0)

# %% DTW ê±°ë¦¬í–‰ë ¬ & ìµœì  k íƒìƒ‰ (ë³€ìˆ˜ min-max scaling)
ts = pivot.dropna(axis=0)
scaler = MinMaxScaler()
ts_norm = ts.apply(lambda row: scaler.fit_transform(row.values.reshape(-1,1)).ravel(), axis=1, result_type="expand")
ts_norm.columns = ts.columns
X = ts_norm.to_numpy()
D = cdist_dtw(X)

def choose_best_k(distance_matrix, k_range=range(2, 9)):
    best_k, best_score, best_labels = None, -1, None
    for k in k_range:
        model = AgglomerativeClustering(
            n_clusters=k, metric="precomputed", linkage="average"
        )
        labels = model.fit_predict(distance_matrix)
        score = silhouette_score(distance_matrix, labels, metric="precomputed")
        print(f"k={k}, silhouette={score:.4f}")
        if score > best_score:
            best_k, best_score, best_labels = k, score, labels
    return best_k, best_score, best_labels

best_k, best_sil, cluster_labels = choose_best_k(D, k_range=range(2, 9))
print("Best k:", best_k, "Silhouette:", best_sil)


# %% ìµœì¢… êµ°ì§‘ ì í•©
final_model = AgglomerativeClustering(
    n_clusters=2, metric="precomputed", linkage="average"
)
cluster_labels = final_model.fit_predict(D)

# ê²°ê³¼ ì €ì¥
cluster_result = pd.DataFrame({
    "í–‰ì •ë™ëª…": ts_norm.index,
    "cluster": cluster_labels
})
print(cluster_result.sort_values("cluster"))


# %% ê·¸ë£¹ë³„ ì„ í˜• íšŒê·€ í•´ë³´ê¸° .. plotì€ ì•ˆê·¸ë ¸ìŒ
dates = ts_norm.columns.to_list()
x_vals = np.arange(len(dates))  # ë¶„ê¸° index
for lab in sorted(set(cluster_labels)):
    cluster_series = ts_norm.iloc[[i for i, lbl in enumerate(cluster_labels) if lbl == lab]]
    # êµ°ì§‘ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ í’€ê¸° (xëŠ” ë¶„ê¸° index ë°˜ë³µ, yëŠ” ê°’ë“¤)
    xs, ys = [], []
    for row in cluster_series.values:
        xs.extend(list(range(len(dates))))
        ys.extend(row.tolist())
    # íšŒê·€ ëª¨ë¸ ì í•©
    X = sm.add_constant(xs)  # ì ˆí¸ ì¶”ê°€
    model = sm.OLS(ys, X).fit()
    print(f"ğŸ“Š Cluster {lab} íšŒê·€ ìš”ì•½")
    print(model.summary())


# %% ì‹œê°í™” (ì  í¬ê¸°=ê·¼ì²˜ ì´ì›ƒ ìˆ˜, ë¹„ëª¨ìˆ˜ì¶”ì„¸ì„ =LOWESS)
for lab in sorted(set(cluster_labels)):
    cluster_series = ts_norm.iloc[[i for i, lbl in enumerate(cluster_labels) if lbl == lab]]

    date_labels = [f"{str(d)[:4]}_{str(d)[-1]}" for d in dates]
    x_vals = np.arange(len(dates)).reshape(-1,1)  # íšŒê·€ìš© xì¶• (ìˆ«ì)
    #ëª¨ë“  ì  ëª¨ìœ¼ê¸°
    xs, ys = [], []
    for row in cluster_series.values:
        xs.extend(list(range(len(dates))))
        ys.extend(row.tolist())
    X_points = np.vstack([xs, ys]).T

    # ë°˜ê²½ r ë‚´ ì´ì›ƒ ê°œìˆ˜ = ì  í¬ê¸°
    r = 0.05  # yì¶• ê°’ ê¸°ì¤€ (ì •ê·œí™” í–ˆìœ¼ë‹ˆ 0~1 ë²”ìœ„ë©´ 0.05~0.1 ì ë‹¹)
    nbrs = NearestNeighbors(radius=r).fit(X_points)
    sizes = np.array([
        len(nbrs.radius_neighbors([pt], return_distance=False)[0]) 
        for pt in X_points
    ]) * 100  # ë°°ìœ¨ ì¡°ì •

    # LOWESS ë¹„ëª¨ìˆ˜ ì¶”ì„¸ì„ 
    # xsëŠ” ë¶„ê¸° ì¸ë±ìŠ¤, ysëŠ” ê°’
    smooth = lowess(ys, xs, frac=0.7)  # frac=0.3: ìŠ¤ë¬´ë”© ì •ë„ (0.1~0.5 ì¡°ì • ê°€ëŠ¥)
    x_smooth, y_smooth = smooth[:,0], smooth[:,1]

    # ì‹œê°í™”
    plt.figure(figsize=(12,6))
    plt.scatter([date_labels[x] for x in xs], ys, s=sizes,
                alpha=0.4, color="skyblue", label="êµ°ì§‘ ì‹œê³„ì—´ ì  (í¬ê¸°=ë°€ì§‘ë„)")
    plt.plot([date_labels[int(x)] for x in x_smooth], y_smooth,
             color="red", linewidth=3, label="LOWESS ì¶”ì„¸ì„ ")
    plt.title(f"Cluster {lab}")
    plt.xlabel("date (ë¶„ê¸°)")
    plt.ylabel("ì§€ì¶œì´ê¸ˆì•¡ (ì •ê·œí™” ê°’)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


# %% ë¡œì§€ìŠ¤í‹± íšŒê·€
from statsmodels.stats.outliers_influence import variance_inflation_factor

data = final.copy()
data = data.merge(cluster_result, on="í–‰ì •ë™ëª…")
y = data["cluster"]
X = data.drop(columns=["í‰ì¼ìœ ë™ì¸êµ¬_ë°€ë„", "ì£¼ë§ìœ ë™ì¸êµ¬_ë°€ë„", "ìƒê¶Œë³€í™”", "ì§€ì¶œì´ê¸ˆì•¡", "í–‰ì •ë™ì½”ë“œ", "í–‰ì •ë™ëª…", "cluster", "date"])

# VIF ê³„ì‚°
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i)
                       for i in range(X.shape[1])]
    return vif_data

vif_table = calculate_vif(X)
print("ì´ˆê¸° VIF:\n", vif_table)
selected_features = vif_table[vif_table["VIF"] < 10]["feature"].tolist()
X_reduced = X[selected_features]

# ë¡œì§€ìŠ¤í‹± íšŒê·€ (statsmodels)
X_reduced = sm.add_constant(X_reduced)
logit_model = sm.Logit(y, X_reduced)
result = logit_model.fit()
summary_table = pd.DataFrame({
    "Estimate": result.params,
    "StdErr": result.bse,
    "z value": result.tvalues,
    "p value": result.pvalues,
    "Odds Ratio": np.exp(result.params)
})
print(summary_table)

# %%
import geopandas as gpd
from libpysal.weights import Queen
from esda.moran import Moran
import spreg

gdf = gpd.read_file("C:/Users/jeongmin/Downloads/vscode/ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì˜ì—­-í–‰ì •ë™)/ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì˜ì—­-í–‰ì •ë™).shp", encoding="utf-8")
df = pd.read_excel("C:/Users/jeongmin/Downloads/vscode/ê°•ì„œêµ¬ìƒê¶Œ.xlsx")
data2 = gdf.merge(df, left_on="ADSTRD_NM", right_on="í–‰ì •ë™ëª…")
data = data.merge(data2, on="í–‰ì •ë™ëª…")

y = data["cluster"].values
X = data[["ìƒí™œìš©í’ˆì§€ì¶œ", "êµìœ¡ì§€ì¶œ", "ì§€í•˜ì² ì—­_ë”ë¯¸", "ìœ ë™ì¸êµ¬_ë°€ë„"]].values

# ê³µê°„ ê°€ì¤‘ì¹˜ í–‰ë ¬ (Queen contiguity: ì¸ì ‘ í–‰ì •ë™)
w = Queen.from_dataframe(data)
w.transform = "r"

# Moranâ€™s I (ì¢…ì†ë³€ìˆ˜ì˜ ê³µê°„ì  ìê¸°ìƒê´€ í™•ì¸)
moran = Moran(y, w)
print("Moranâ€™s I:", moran.I, "p-value:", moran.p_sim)

# ê³µê°„ íšŒê·€ ëª¨í˜• (Spatial Lag)
model_lag = spreg.ML_Lag(y, X, w=w, name_y="cluster", 
                         name_x=["ìƒí™œìš©í’ˆì§€ì¶œ", "êµìœ¡ì§€ì¶œ", "ì§€í•˜ì² ì—­_ë”ë¯¸", "ìœ ë™ì¸êµ¬_ë°€ë„"])
print(model_lag.summary)

# ì˜ˆì¸¡ê°’ì„ GeoDataFrameì— ì¶”ê°€
data["pred_cluster"] = model_lag.predy.flatten()

#%%
from shapely.geometry import Point
# data ìì²´ê°€ GeoDataFrameì¸ì§€ ë³´ì •
data = gpd.GeoDataFrame(data, geometry="geometry", crs=gdf.crs)

# threshold ê¸°ì¤€ìœ¼ë¡œ high_prob_areas ì¶”ì¶œ
threshold = data["pred_cluster"].quantile(0.95)
high_prob_areas = data[data["pred_cluster"] >= threshold].copy()

# ë‹¤ì‹œ GeoDataFrameìœ¼ë¡œ ë³€í™˜ (geometry ìœ ì§€)
high_prob_areas = gpd.GeoDataFrame(high_prob_areas, geometry="geometry", crs=gdf.crs)

# buffer (500m)
high_prob_buffer = high_prob_areas.buffer(300)

# ìƒê¶Œì ë„ GeoDataFrameìœ¼ë¡œ ë³€í™˜ (ì¢Œí‘œê³„ ë™ì¼í•˜ê²Œ)
df["geometry"] = df.apply(lambda row: Point(row["ì—‘ìŠ¤ì¢Œí‘œ_ê°’"], row["ì™€ì´ì¢Œí‘œ_ê°’"]), axis=1)
store_gdf = gpd.GeoDataFrame(df, geometry="geometry", crs=gdf.crs)

# buffer ë‚´ë¶€ì˜ ìƒê¶Œë§Œ ì¶”ì¶œ
selected_stores = store_gdf[store_gdf.within(high_prob_buffer.union_all())]

# %%
data_unique = data.drop_duplicates(subset=["í–‰ì •ë™ëª…"]).reset_index(drop=True)
fig, ax = plt.subplots(figsize=(10,8))
data_unique.plot(
    column="pred_cluster",
    cmap="RdYlGn_r",
    legend=True,
    ax=ax,
)
plt.title("ìƒê¶Œ ê³ ë³€ë™ í™•ë¥  ì§€ë„", fontsize=14)
plt.axis("off")
plt.show()


# -------------------------------
# ì§€ë„ 2: ìµœì¢… í›„ë³´ ìƒê¶Œ ì§€ë„
# -------------------------------
fig, ax = plt.subplots(figsize=(10,8))
# ë°°ê²½ (ëª¨ë“  í–‰ì •ë™ íšŒìƒ‰)
data.plot(ax=ax, color="lightgrey", edgecolor="black")
# ê³ í™•ë¥  ì§€ì—­ (ì˜¤ë Œì§€)
high_prob_areas.plot(ax=ax, color="orange", alpha=0.5, edgecolor="red", label="ê³ í™•ë¥  ì§€ì—­")
# ì„ ì •ëœ ìƒê¶Œ (íŒŒë€ ì )
selected_stores.plot(ax=ax, color="blue", markersize=30, label="ì„ ì •ëœ ìƒê¶Œ")

plt.legend()
plt.title("ìµœì¢… ì„ ì •ëœ ìƒê¶Œ í›„ë³´", fontsize=14)
plt.axis("off")
plt.show()


# %%
